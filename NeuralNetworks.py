# -*- coding: utf-8 -*-
"""Assignment2_Part2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O5D2JKaGJkZeeie4kq1j8CQukHa9c5d8
"""

pip install ucimlrepo

#####################################################################################################################
#   Assignment 2: Neural Network Analysis
#   This is a starter code in Python 3.6 for a neural network.
#   You need to have numpy and pandas installed before running this code.
#   You need to complete all TODO marked sections
#   You are free to modify this code in any way you want, but need to mention it
#       in the README file.
#
#####################################################################################################################

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.metrics import accuracy_score, mean_squared_error, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
from itertools import product
import warnings
warnings.filterwarnings('ignore')

class NeuralNet:
    def __init__(self, dataFile, header=True):
        self.raw_input = pd.read_csv(dataFile)
        self.processed_data = None
        self.scaler = StandardScaler()
        self.label_encoders = {}
        self.is_classification = True
        self.results = []
        self.model_histories = {}

    def preprocess(self):
        """
        Pre-processing the dataset including:
        - Handling missing values
        - Standardization/normalization
        - Categorical to numerical conversion
        - Data type optimization
        """
        print("Starting data preprocessing...")
        print(f"Original data shape: {self.raw_input.shape}")

        # Make a copy to avoid modifying original data
        self.processed_data = self.raw_input.copy()

        # Display basic information about the dataset
        print("\nDataset Info:")
        print(self.processed_data.info())
        print("\nFirst few rows:")
        print(self.processed_data.head())

        # Handle missing values
        print(f"\nMissing values before handling:")
        print(self.processed_data.isnull().sum())

        # For numerical columns, fill missing values with median
        numerical_cols = self.processed_data.select_dtypes(include=[np.number]).columns
        for col in numerical_cols:
            if self.processed_data[col].isnull().sum() > 0:
                self.processed_data[col].fillna(self.processed_data[col].median(), inplace=True)

        # For categorical columns, fill missing values with mode
        categorical_cols = self.processed_data.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            if self.processed_data[col].isnull().sum() > 0:
                self.processed_data[col].fillna(self.processed_data[col].mode()[0], inplace=True)

        # Convert categorical variables to numerical
        for col in categorical_cols:
            if col != self.processed_data.columns[-1]:
                le = LabelEncoder()
                self.processed_data[col] = le.fit_transform(self.processed_data[col])
                self.label_encoders[col] = le

        # Handle target variable (last column)
        target_col = self.processed_data.columns[-1]
        if self.processed_data[target_col].dtype == 'object':
            le = LabelEncoder()
            self.processed_data[target_col] = le.fit_transform(self.processed_data[target_col])
            self.label_encoders[target_col] = le
            self.is_classification = True
        else:
            # Check if it's a continuous target (regression) or discrete (classification)
            unique_values = self.processed_data[target_col].nunique()
            if unique_values > 10:  # Assume regression if more than 10 unique values
                self.is_classification = False
            else:
                self.is_classification = True

        print(f"\nMissing values after handling:")
        print(self.processed_data.isnull().sum())
        print(f"Processed data shape: {self.processed_data.shape}")
        print(f"Task type: {'Classification' if self.is_classification else 'Regression'}")

        return 0

    def train_evaluate(self):
        """
        Train and evaluate models for all combinations of parameters.
        Outputs:
        1. Training Accuracy and Error (Loss) for every model
        2. Test Accuracy and Error (Loss) for every model
        3. History Curve (Plot of Accuracy against training steps) for all models
        """
        print("\nStarting model training and evaluation...")

        # Prepare data
        ncols = len(self.processed_data.columns)
        nrows = len(self.processed_data.index)
        X = self.processed_data.iloc[:, 0:(ncols - 1)]
        y = self.processed_data.iloc[:, (ncols-1)]

        # Split the data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y if self.is_classification else None)

        # Standardize features
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)

        print(f"Training set shape: {X_train_scaled.shape}")
        print(f"Test set shape: {X_test_scaled.shape}")

        # Hyperparameters as specified in the assignment
        activations = ['logistic', 'tanh', 'relu']
        learning_rates = [0.01, 0.1]
        max_iterations = [100, 200]  # also known as epochs
        num_hidden_layers = [2, 3]

        # Fixed number of neurons per hidden layer
        neurons_per_layer = 50

        # Generate all combinations of hyperparameters
        param_combinations = list(product(activations, learning_rates, max_iterations, num_hidden_layers))

        print(f"\nTotal number of model combinations: {len(param_combinations)}")

        # Initialize results storage
        self.results = []

        # Train and evaluate each model
        for i, (activation, lr, max_iter, n_hidden) in enumerate(param_combinations):
            print(f"\nTraining model {i+1}/{len(param_combinations)}")
            print(f"Parameters: activation={activation}, lr={lr}, max_iter={max_iter}, hidden_layers={n_hidden}")

            # Create hidden layer sizes tuple
            hidden_layer_sizes = tuple([neurons_per_layer] * n_hidden)

            try:
                # Create model based on task type
                if self.is_classification:
                    model = MLPClassifier(
                        hidden_layer_sizes=hidden_layer_sizes,
                        activation=activation,
                        learning_rate_init=lr,
                        max_iter=max_iter,
                        random_state=42,
                        early_stopping=False,
                        warm_start=False
                    )
                else:
                    model = MLPRegressor(
                        hidden_layer_sizes=hidden_layer_sizes,
                        activation=activation,
                        learning_rate_init=lr,
                        max_iter=max_iter,
                        random_state=42,
                        early_stopping=False,
                        warm_start=False
                    )

                # Train the model
                model.fit(X_train_scaled, y_train)

                # Make predictions
                y_train_pred = model.predict(X_train_scaled)
                y_test_pred = model.predict(X_test_scaled)

                # Calculate metrics
                if self.is_classification:
                    train_accuracy = accuracy_score(y_train, y_train_pred)
                    test_accuracy = accuracy_score(y_test, y_test_pred)
                    train_error = 1 - train_accuracy
                    test_error = 1 - test_accuracy
                    train_mse = mean_squared_error(y_train, y_train_pred)
                    test_mse = mean_squared_error(y_test, y_test_pred)
                else:
                    train_mse = mean_squared_error(y_train, y_train_pred)
                    test_mse = mean_squared_error(y_test, y_test_pred)
                    train_accuracy = None
                    test_accuracy = None
                    train_error = train_mse
                    test_error = test_mse

                # Store results
                result = {
                    'model_id': i+1,
                    'activation': activation,
                    'learning_rate': lr,
                    'max_iterations': max_iter,
                    'hidden_layers': n_hidden,
                    'hidden_layer_sizes': hidden_layer_sizes,
                    'train_accuracy': train_accuracy,
                    'test_accuracy': test_accuracy,
                    'train_error': train_error,
                    'test_error': test_error,
                    'train_mse': train_mse,
                    'test_mse': test_mse,
                    'n_iterations': model.n_iter_,
                    'loss_curve': model.loss_curve_
                }

                self.results.append(result)

                # Store model history for plotting
                model_name = f"Model_{i+1}_{activation}_{lr}_{max_iter}_{n_hidden}L"
                self.model_histories[model_name] = {
                    'loss_curve': model.loss_curve_,
                    'iterations': list(range(1, len(model.loss_curve_) + 1))
                }

                print(f"Training completed successfully!")
                if self.is_classification:
                    print(f"Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}")
                else:
                    print(f"Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}")

            except Exception as e:
                print(f"Error training model {i+1}: {str(e)}")
                continue

        # Generate outputs
        self._generate_results_table()
        self._plot_model_histories()
        self._generate_summary_report()

        return 0

    def _generate_results_table(self):
        """Generate and display results table"""
        print("\n" + "="*100)
        print("RESULTS TABLE")
        print("="*100)

        # Create DataFrame from results
        df_results = pd.DataFrame(self.results)

        # Select columns for display
        if self.is_classification:
            display_cols = ['model_id', 'activation', 'learning_rate', 'max_iterations',
                          'hidden_layers', 'train_accuracy', 'test_accuracy',
                          'train_error', 'test_error', 'train_mse', 'test_mse']
        else:
            display_cols = ['model_id', 'activation', 'learning_rate', 'max_iterations',
                          'hidden_layers', 'train_mse', 'test_mse']

        display_df = df_results[display_cols].round(4)

        print(display_df.to_string(index=False))

        # Save to CSV
        display_df.to_csv('model_results.csv', index=False)
        print(f"\nResults saved to 'model_results.csv'")

        # Find best model
        if self.is_classification:
            best_model_idx = df_results['test_accuracy'].idxmax()
            best_metric = 'test_accuracy'
            best_value = df_results.loc[best_model_idx, 'test_accuracy']
        else:
            best_model_idx = df_results['test_mse'].idxmin()
            best_metric = 'test_mse'
            best_value = df_results.loc[best_model_idx, 'test_mse']

        print(f"\nBest model: Model {df_results.loc[best_model_idx, 'model_id']}")
        print(f"Best {best_metric}: {best_value:.4f}")
        print(f"Parameters: {df_results.loc[best_model_idx, ['activation', 'learning_rate', 'max_iterations', 'hidden_layers']].to_dict()}")

    def _plot_model_histories(self):
        """Plot model history curves"""
        if not self.model_histories:
            print("No model histories to plot.")
            return

        # Create multiple plots to avoid congestion
        models_per_plot = 6
        n_plots = len(self.model_histories) // models_per_plot + (1 if len(self.model_histories) % models_per_plot > 0 else 0)

        model_names = list(self.model_histories.keys())

        for plot_idx in range(n_plots):
            plt.figure(figsize=(12, 8))

            start_idx = plot_idx * models_per_plot
            end_idx = min((plot_idx + 1) * models_per_plot, len(model_names))

            for i in range(start_idx, end_idx):
                model_name = model_names[i]
                history = self.model_histories[model_name]

                plt.plot(history['iterations'], history['loss_curve'],
                        label=model_name, linewidth=2, alpha=0.8)

            plt.xlabel('Epochs')
            plt.ylabel('Loss')
            plt.title(f'Model Training History - Part {plot_idx + 1}')
            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            plt.grid(True, alpha=0.3)
            plt.tight_layout()

            # Save plot
            plt.savefig(f'model_history_part_{plot_idx + 1}.png', dpi=300, bbox_inches='tight')
            plt.show()

        # Create a combined plot with all models (may be congested but requested)
        plt.figure(figsize=(15, 10))

        colors = plt.cm.tab20(np.linspace(0, 1, len(self.model_histories)))

        for i, (model_name, history) in enumerate(self.model_histories.items()):
            plt.plot(history['iterations'], history['loss_curve'],
                    label=model_name, color=colors[i], linewidth=1.5, alpha=0.7)

        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.title('All Models Training History')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()

        # Save combined plot
        plt.savefig('all_models_history.png', dpi=300, bbox_inches='tight')
        plt.show()

    def _generate_summary_report(self):
        """Generate summary report"""
        print("\n" + "="*100)
        print("SUMMARY REPORT")
        print("="*100)

        df_results = pd.DataFrame(self.results)

        # Analysis by activation function
        print("\nPerformance by Activation Function:")
        if self.is_classification:
            activation_performance = df_results.groupby('activation')['test_accuracy'].agg(['mean', 'std', 'max', 'min'])
            print(activation_performance.round(4))

            best_activation = df_results.loc[df_results['test_accuracy'].idxmax(), 'activation']
            print(f"\nBest performing activation function: {best_activation}")

        else:
            activation_performance = df_results.groupby('activation')['test_mse'].agg(['mean', 'std', 'max', 'min'])
            print(activation_performance.round(4))

            best_activation = df_results.loc[df_results['test_mse'].idxmin(), 'activation']
            print(f"\nBest performing activation function: {best_activation}")

        # Analysis by learning rate
        print("\nPerformance by Learning Rate:")
        if self.is_classification:
            lr_performance = df_results.groupby('learning_rate')['test_accuracy'].agg(['mean', 'std', 'max', 'min'])
        else:
            lr_performance = df_results.groupby('learning_rate')['test_mse'].agg(['mean', 'std', 'max', 'min'])
        print(lr_performance.round(4))

        # Analysis by number of hidden layers
        print("\nPerformance by Number of Hidden Layers:")
        if self.is_classification:
            layers_performance = df_results.groupby('hidden_layers')['test_accuracy'].agg(['mean', 'std', 'max', 'min'])
        else:
            layers_performance = df_results.groupby('hidden_layers')['test_mse'].agg(['mean', 'std', 'max', 'min'])
        print(layers_performance.round(4))

        # Save detailed report
        with open('detailed_report.txt', 'w') as f:
            f.write("Neural Network Analysis Report\n")
            f.write("="*50 + "\n\n")
            f.write(f"Dataset: tripadvisor_review.csv\n")
            f.write(f"Task Type: {'Classification' if self.is_classification else 'Regression'}\n")
            f.write(f"Total Models Trained: {len(self.results)}\n\n")

            f.write("Hyperparameter Analysis:\n")
            f.write("-" * 25 + "\n")
            f.write(f"Activation functions tested: {list(df_results['activation'].unique())}\n")
            f.write(f"Learning rates tested: {list(df_results['learning_rate'].unique())}\n")
            f.write(f"Max iterations tested: {list(df_results['max_iterations'].unique())}\n")
            f.write(f"Hidden layer configurations tested: {list(df_results['hidden_layers'].unique())}\n\n")

            if self.is_classification:
                f.write(f"Best Model Performance:\n")
                f.write(f"Best Test Accuracy: {df_results['test_accuracy'].max():.4f}\n")
                f.write(f"Best Model Configuration: {df_results.loc[df_results['test_accuracy'].idxmax(), ['activation', 'learning_rate', 'max_iterations', 'hidden_layers']].to_dict()}\n")
            else:
                f.write(f"Best Model Performance:\n")
                f.write(f"Best Test MSE: {df_results['test_mse'].min():.4f}\n")
                f.write(f"Best Model Configuration: {df_results.loc[df_results['test_mse'].idxmin(), ['activation', 'learning_rate', 'max_iterations', 'hidden_layers']].to_dict()}\n")

        print("\nDetailed report saved to 'detailed_report.txt'")

if __name__ == "__main__":
    neural_network = NeuralNet("tripadvisor_review.csv")  # Updated to use your dataset
    neural_network.preprocess()
    neural_network.train_evaluate()